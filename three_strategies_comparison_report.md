# 大模型 MoE 显存卸载缓存策略演进报告：从空间分布到时序轨迹的探索

本研究旨在解决资源极端受限的单节点环境中运行超大混合专家模型（如 Mixtral-8x7B）时的“由于频繁 Swap 导致的 I/O 带宽墙 (I/O Wall)”问题。随着测试和算法设计的不断深入，我们尝试了三种代际递进的思想对底层缓存（Cache）进行彻底重构。

以下是我们的核心实验结论和三种算法的横向比对：

---

## 策略一：协同过滤 —— 基于门控分布概率的无向图 (Spatial Gating Graph)
> EBCO (Expert-Based Collaborative Offloading) 最初试图通过专家之间的“抱团取暖”来缓解显存错失。

### 1. 核心理论模型与实现
- **思想来源**：**空间局部性 (Spatial Locality)**。我们在分析路由模块时假设，当模型在处理某一段特定语义（如“生成 Python 代码”）时，经常会在层内部激活固定的两个专家 (A 和 B)。
- **工程实现**：我们提取了 Router 决策出来的 Top-K 分布概率，在 CPU 端动态维护一张基于层内的 $8 \times 8$ 无向概率图。当发生缓存未命中时，系统优先保全那些在图谱中拥有更高“协同防守得分”的死党专家，驱逐分数最低的冷门专家。

### 2. 实验结果与反思
*   **最佳成绩 (Hit Rate)**: **34.67%** （甚至略低于基础 LRU 队列的 36.86%）
*   **败因定性**：**马后炮陷阱 (Reactive Over-fitting) 与 PCIe 独占**。
    门控概率是在“木已成舟”时才被计算出来的滞后指标。当大模型在层内发生“语义跳变”（从中文环境跨入代码环境）时，原有的死党专家图无法预测这一变故，反而在本该需要新专家时，死死捂住了无用的旧有防守联盟阵地。且由于缺乏对“下一层”的前置跨层预取量（Lookahead），其性能与普通的最近最少使用 (LRU) 并无二致。

---

## 策略二：向量法 —— 基于 Embedding 与时空运动轨迹 (Trajectory Lookahead)
> 既然“概率结果”不可靠也来不及，我们把目光刺入了“一切决策背后的原因”—— Token 的万维隐状态空间分布。

### 1. 核心理论模型与实现
- **思想来源**：**高维连续空间的轨迹惯性 (Continuous Trajectory)**。
- **工程实现**：我们在系统预热段，为模型的每一层 8 个专家各自捕捉出了一个能代表其“口味”的语义质心位置（Centroid / 4096维向量）。
  在随后的前向传播 (Forward) 过程中，我们在系统进入繁重的算力盲区（如纯 MLP 乘加计算块）前，立刻抽取当前正在层级中穿梭的 Token 的 `hidden_states`。将其视作宇宙中的飞船，跨层计算其落点最接近的下一层引力场（专家分类超平面/质心距离）。并悄悄在 PCIe 后台发射异频预判指令，抢先从 CPU 取回下一层注定要使用的专家。

### 2. 实验结果与反思
*   **最佳成绩 (Hit Rate)**: **43.93% 🔥** （突破性数据，击穿 36% 基线！）
*   **败因定性**：**高维算法带来的 CPU 路由系统崩塌**。
    这证明了理论的绝对胜利：基于高维语境特征的跨层余弦相似度（Cosine Similarity）能够以极微的容错率预判未来的轨迹。然而在小并发自回归生成、且受制于 Python GIL 特性的单卡工程上，频繁触发这些重型张量通讯与循环调度（动辄 30 秒的 CPU 同步耗时），掩盖了其本应带来的流水线效益，使得 TPS 被高昂的算法控制平面开销完全吞没。

---

## 策略三：有向图法 —— 基于时序马尔可夫链转移预判 (Temporal Markov Chain)
> 我们吸取了前两者的血泪教训：要像模型一样拥有“时间局部性规律”，还要拥有 $O(1)$ 的极简计算开销。

### 1. 核心理论模型与实现
- **思想来源**：**自回归语言模型的有向时间单箭头 (Directed Temporal Locality)**。
- **工程实现**：彻底舍弃层级内那些“无方向的同盟关系”，也舍弃繁重的高维 Embedding 距离计算。模型在内存维护了一个极致瘦身的三维概率浮点矩阵：`P[层号, 当前Token激活专家, 下一Token激活专家]`。
  当 Token T 使用专家 A 完毕，其向 Token T+1 使用专家 B 转化的事件被系统捕获，概率权重 `+1`。
  当新的推理步伐到来，模型通过纯 $O(1)$ 的查表机制，瞬间锁定当前活跃专家的“未来有向概率最高走向”，并在 GPU 开始疯狂计算 $L$ 层前，用 0 开销向底层驱动下放非阻塞拷贝指令 (`non_blocking=True`)。

### 2. 实验结果与反思
*   **最佳成绩 (Hit Rate)**: **44.23% 🔥🥇** （全场冠军）
*   **学术揭底 (工程之困)**：**CUDA 异步拷贝指令队列锁死 (Queue Saturation)**。
    有向转移图 (Markov Chain) 实现了理论与逻辑的统一：不费吹灰之力的 $O(1)$ 代价，拿到了全场最高的预知拦截率！但其依然在 TPS 的提升上折戟（仅有极速预判，耗时却未缩减）。由于马尔可夫预判太快、下单太频，导致本就只能吞吐 64GB/s 左右且被串行化的底层 PCIe DMA “取货窗口”排满了数百张未执行的微小内存搬运支票，直接塞爆了显卡驱动的指令流系统（CUDA Copy Command Queue），最终反噬阻塞了 Python 主线程的步伐。

---

## 终极研究结论与落地建议 

经过三代算法从静态空间到动态时序的生死迭代，我们通过实验得到了单卡极致大模型推理的铁则：

1. **时序远大于空间**：大语言模型的缓存命中率决胜局在跨 Token 的有向图预测，而非跨层或同层的静态绑定防守。
2. **纯调度性算法面临底层瓶颈**：面对绝对的窄桥（PCIe 带宽），在单机由于资源极致干涸而必须频繁“一进一出”的恶劣环境中，通过软件应用层（哪怕是 $O(1)$ 读取）向 CUDA 下达细碎并发指令试图压榨管线潜能，都必将导致极其严重的底层队列锁死。

**向未来的建议 (Roadmap)**:
基于“不改变模型自身（不进行量化、蒸馏或剪枝）”的严苛学术与工程约束，我们必须在纯 **系统工程 (Systems Engineering)** 层面跨越这道终极 I/O 鸿沟：

1. **异步并发流限流 (DMA Queue Throttling)**：
    针对有向图马尔可夫策略引发的 CUDA 指令池爆仓，设计一个“流量控制阀”。通过 `torch.cuda.Event` 或专用的 background stream，保证同时在 PCIe 数据总线上排队的预取任务不超过 1-2 个。这能在保障 O(1) 前瞻预测命中率的同时，彻底释放 Python 主线程被锁死的 30 秒开销。

2. **微架构级的多缓存队列 (Tiered Buffer Management)**：
    不改变专家大小，但改变显存的流转方式。通过 Pinned Memory (Page-locked CPU Ram) 把所有专家的内存锁死，利用纯 C++ 层的异步线程池替换 PyTorch `non_blocking=True` API 带来的 Python GIL 锁死问题。

3. **细粒度物理拆分传输 (Fine-grained Block Transfer)**: 
    模型结构不改变但传输结构可以改变。在底层驱动级别，将单个 1.5GB 的专家张量在传输时切割为 100MB 级别的小块，利用真正的 Pipeline（算前 100MB 资源时，搬运后 100MB 资源）来消除尖峰拥堵。
